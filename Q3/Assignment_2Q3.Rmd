---
title: 'Steven Maharaj 695281 Assignment 2, Question 3 MAST90125: Bayesian Statistical Learning'
header-includes:
   - \usepackage{bm}
   - \usepackage{amsmath}
   - \usepackage{amsfonts}
   - \usepackage{amssymb}
output: 
  pdf_document:
    number_sections: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Due: Friday 20 September 2019**  
\vspace{5 mm}

**There are places in this assignment where R code will be required. Therefore set the random seed so assignment is reproducible.**

```{r}
set.seed(695281)  #Please change random seed to your student id number.
library(dplyr)
library(ggplot2)
library(mvtnorm)
library(coda)
```

## Question Three (18 marks)

A group of 453 Bangladeshi women in 5 districts were asked about contraceptive use. The response variable *use* is an indicator for contraceptive use (coded N for no and Y for yes). Other covariates of interest are categorical variables for geographical location *district* (5 levels), and *urban* (2 levels), and number of living children *livch* (4 levels), and the continuous covariate for standardised age *age*.A random intercept for the district was suggested. This suggested the following model should be fitted,

\[ \bm \theta = {\bf Z}{\bf u} + {\bf X}{\bm \beta},  \]

where $\bm \theta$ is a link function, $\bf Z$ is an indicator variable for district, $\bf u$ is a random intercept with prior $p({\bf u}) = \mathcal{N}({\bf 0},\sigma^2_u{\bf I})$, and ${\bf X}$ is a design matrix for fixed effects $\bm \beta$, where $\bm \beta$ includes the coefficients for the intercept, urban status, living children, and age. 

Data can be downloaded from LMS as \texttt{Contraceptionsubset.csv}.

a) Fit a generalised linear mixed model assuming a logistic link using \texttt{Stan}. The R and stan code below covers the following steps.

\begin{itemize}
\item Importing the data.
\item Constructing design matrices.
\item Provides code to go into the stan file.
\item Running stan in R. This assumes your stan file is called *logitmm.stan*, and that you will run the sampler for 2000 iterations and 4 chains.
\end{itemize}

Note that provided code assumes everything required is located in your working directory in R.

```{r}
#Step one: Importing data,  constructing design matrices and calculating matrix dimensions.
dataX= read.csv("Contraceptionsubset.csv",header=TRUE)

n<-dim(dataX)[1]
Z    = table(1:n,dataX$district)     #incidence matrix for district
Q    = dim(Z)[2]
D1   = table(1:n,dataX$livch) #Dummy indicator for living children
D2   = table(1:n,dataX$urban) #Dummy indicator for urban status

#fixed effect design matrix 
X    = cbind(rep(1,n),dataX$age,D1[,-1],D2[,-1])
P    = dim(X)[2]
y    = rep(0,n)
y[dataX$use %in% 'Y'] = 1
```


An example stan file.
\begin{footnotesize}
\begin{verbatim}
//
// This Stan program defines a logistic mixed model
//
// Learn more about model development with Stan at:
//
//    http://mc-stan.org/users/interfaces/rstan.html
//    https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started
//

data {
  int<lower=0> n;   //number of observations
  int<lower=0> Q;   //number of random effect levels
  int<lower=0> P;   //number of fixed effect levels
  int y[n];      //response vector
  matrix[n,Q] Z;     //indicator matrix for random effect levels
  matrix[n,P] X;     //design matrix for fixed effects
}

// The parameters accepted by the model. 
// accepts three sets of parameters 'beta', 'u' and 'sigma'.
parameters {
  vector[P] beta; //vector of fixed effects of length P.
  vector[Q] u; //vector of random effects of length Q.
  real<lower=0> sigma; //random effect standard deviation
}


// The model to be estimated. We model the output
// 'y' to be bernoulli with logit link function, 
// and assume a i.i.d. normal prior for u.
model {
  u ~ normal(0,sigma);                //prior for random effects.
  y ~ bernoulli_logit(X*beta+ Z*u);  //likelihood
  
}
\end{verbatim}
\end{footnotesize}

```{r}
library(rstan)
logistic.mm <-stan(file="logitmm.stan",data=c('Z','X','y','n','P','Q'),iter=2000,chains=4)
print(logistic.mm)
```


Note that in Stan, defaults for burn-in (warm-up) is one half of all iterations in stan, and no thinning. Note the code is written using the stan file and csv is in your working directory. Use the \texttt{print} function to report posterior means, standard deviations, 95 \% central credible intervals and state from the output whether you believe the chains have converged. Also report the reference categories for  *urban* and *livch*.


Reporting for PART A:


Posterior means, standard deviations can be found in the above table. The lower limit of the 95 \% central credible intervals given by the column labeled "2.5\%" while the  upperer limit of the 95 \% central credible intervals given by the column labeled "97.5\%". 
```{r}
print(summary(logistic.mm)$summary)
```

Using the Gelman-Rubin diagnostic (Rhat) only 6 out of the 12 parameters These were beta[2],beta[3],beta[4],beta[5],beta[6],sigma had an Rhat value close to 1. All u parameters and beta[1] do not appear to converge thus the chain did not converge.

For *urban* and *livch* the reference categories are "N" and "0" respectivley. This is infered from the way the DataFrame X was constructed. 
`` X = cbind(rep(1,n),dataX$age,D1[,-1],D2[,-1])``



b) An alternative to the logit link when analysing binary data is the probit. The probit link is defined as,

\begin{eqnarray}
y_i &=& \begin{cases} 1 & \text{if $z_i \geq 0$}\\ 
0 & \text{if $z_i < 0$}\\ 
\end{cases} \nonumber \\
z_i &=& {\bf x}_i'\bm \beta + \epsilon_i, \quad \epsilon \sim \mathcal{N}(0,1). \nonumber 
\end{eqnarray}

In lecture 14, we showed how by letting $z_i$ be normal, probit regression can be fitted using a Gibbs sampler, but to do so, it requires the ability to sample from a truncated normal defined on either $(-\infty,0)$ (if $y_i = 0$) or $(0,\infty)$ (if $y_i = 1$). Check by comparing the empirical and the true density that a modified version of the inverse cdf method can be used to produce draws from a truncated normal. Do this for the case where $x \in (0,\infty)$ and $x \in (-\infty,0)$ with parameters $\mu=0.5$ and $\sigma=1$.

Hints: If $y$ is drawn from a truncated normal with lower bound $a$, upper bound $b$ and parameters $\mu, \sigma^2$ then then $p(y|\mu,\sigma^2,a,b)$ is 

\[ \frac{\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(y-\mu)^2/2}}{\int_{-\infty}^b \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(y-\mu)^2/2} dy - \int_{-\infty}^a \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(y-\mu)^2/2} dy}, \]

which in \texttt{R} means the truncated normal density can be written as 

\begin{footnotesize}
\begin{verbatim}
dnorm(x,mean=mu,sd=sigma)/(pnorm(b,mean=mu,sd=sigma)-pnorm(a,mean=mu,sd=sigma))
\end{verbatim}
\end{footnotesize}

The inverse cdf method involves drawing $v$ from $U(0,1)$ so that $x \sim p(x)$ can be found solving $x=F^{-1}(x)$, where $F$ is the cdf. If the only change compared to drawing from a normal distribution is truncation, think about what happens to the bounds of the uniform distribution. 


Answer:
Part B

Case $x \in (0,\infty)$
```{r}
rtn <- function(n,b,a,mu,Sigma){
  u <- runif(n)
  g <- pnorm((b-mu)/Sigma) - pnorm((a-mu)/Sigma)
  x <-qnorm((g) * u + pnorm((a-mu)/Sigma))*Sigma + mu 
}



x<- rtn(n=100000,b=100,a=0,mu = 0.5,Sigma = 1)

mu = 0.5
Sigma = 1
b=100
a=0


x<-sort(x)
true_den <- dnorm((x-mu)/Sigma)/(pnorm((b-mu)/Sigma) - pnorm((a-mu)/Sigma))

plot(density(x),col = 1,main="X > 0")
lines(x,true_den,col = 2,type = "l")
legend('topright',legend=c('Simulated','True'),col=1:2,lty=1,bty='n')

```



Case $x \in (-\infty,0)$
```{r}
x<- rtn(n=100000,b=0,a=-100,mu = 0.5,Sigma = 1)

mu = 0.5
Sigma = 1
b=0
a=-100


x<-sort(x)
true_den <- dnorm((x-mu)/Sigma)/(pnorm((b-mu)/Sigma) - pnorm((a-mu)/Sigma))

plot(density(x),col = 1,main="X < 0")
lines(x,true_den,col = 2,type = "l")
legend('topleft',legend=c('Simulated','True'),col=1:2,lty=1,bty='n')
```


c) Implement a Gibbs sampler to fit the same mixed model as fitted in Stan in a), but now with a probit link. As before, fit 4 chains, each running for 2000 iterations, with the first 1000 iterations discarded as burn-in. Perform graphical convergence checks and Gelman-Rubin diagnostics. Report posterior means, standard deviations and 95 \% central credible intervals for $\sigma, \bm \beta, {\bf u}$ by combining chains. 




We implement a Gibbs sampler to fit the same mixed model, but now with a probit link.

Assumsing,

 - $p(\bm \beta) \propto 1$
 - $p(\bm u) = \mathcal{N}(\bm 0,\sigma^2_u \bm I)$
 - $p(\tau_u) = Ga(\alpha_u,\gamma_u)$
 
 It can be shown that we have the following conditional posteriors
 
 $$p\left(\tau_{u} | \cdot\right)=\mathrm{Ga}\left(\alpha_{u}+q / 2, \gamma_{u}+\mathbf{u}^{\prime} \mathbf{u} / 2\right)$$

$$
\left.p\left(\left(\begin{array}{c}{\beta} \\ {u}\end{array}\right) | \cdot\right)=\mathcal{N}\left(\begin{array}{cc}{X^{\prime} X} & {X^{\prime} Z} \\ {Z^{\prime} X} & {Z^{\prime} Z+\tau_{u}  \bm{I}^{-1}}\end{array}\right)^{-1}\left(\begin{array}{c}{X^{\prime} z} \\ {Z^{\prime} z}\end{array}\right),\left(\begin{array}{cc}{X^{\prime} X} & {X'Z} \\ {Z^{\prime} X} & {Z'Z+\tau_{u}  \bm{I}^{-1}}\end{array}\right)^{-1}\right)
$$


We define our inputs for the Gibbs Sampler


```{r}
#Step one: Importing data,  constructing design matrices and calculating matrix dimensions.
dataX= read.csv("Contraceptionsubset.csv",header=TRUE)
n<-dim(dataX)[1]
Z    = table(1:n,dataX$district)     #incidence matrix for district
Q    = dim(Z)[2]
D1   = table(1:n,dataX$livch) #Dummy indicator for living children
D2   = table(1:n,dataX$urban) #Dummy indicator for urban status

#fixed effect design matrix 
X    = cbind(rep(1,n),dataX$age,D1[,-1],D2[,-1])
P    = dim(X)[2]
y    = rep(0,n)
y[dataX$use %in% 'Y'] = 1
a <- 0.01
g <- 0.01
# iter =2000
```

Construct a Gibbs sampler
```{r}
Gibbsq3 <- function(iter,Z,X,y,burnin,tauu_0,a,g){
  q    = dim(Z)[2]
  p    = dim(X)[2]
  
  W<-cbind(X,Z)            #for the joint conditional posterior for b,u
  WTW <-crossprod(W)
  I0 <- diag(p+q)
  diag(I0)[1:p] <- 0
  
  #starting values.
  t_u <- tauu_0
  u   <-rnorm(q,0,sd=1/sqrt(t_u))
  b   <-rnorm(p,0,sd=1/sqrt(t_u))
  
  #storing results.
  par <-matrix(0,iter,p+q+1)  
  
  for (i in 1:iter) {
    Prec <-WTW + t_u*I0
    t_u <- rgamma(1,a + q*0.5,g + crossprod(u)*0.5)
    z <- rtn(n=length(y),b=100,a=0,mu = crossprod(t(X),b)+crossprod(t(Z),u),Sigma = 1)*(y==1) + rtn(n=length(y),b=0,a=-100,mu = crossprod(t(X),b)+crossprod(t(Z),u),Sigma = 1)*(y==0) 
    z[is.nan(z)] = 0
    z[is.infinite(z)] = 0
    P.mean <- solve(Prec)%*%crossprod(W,z)
    P.var  <-solve(Prec)
    res <- rmvnorm(1,P.mean,P.var)
    b <- res[1:p]
    u <- res[p+1:q]
    par[i,]<-c(b,u,1/t_u)
  }
par <-par[-c(1:burnin),] #removing initial iterations
colnames(par)<-c(paste('beta',1:p,sep=''),paste('u',1:q,sep=''),"sigma2_u")  
 return(par)
}
```

```{r}

chain1 <- Gibbsq3(iter=2000,Z=Z,X=X,y=y,burnin=1000,tauu_0 = 1,a=a,g=g)
chain2 <- Gibbsq3(iter=2000,Z=Z,X=X,y=y,burnin=1000,tauu_0 = 0.5,a=a,g=g)
chain3 <- Gibbsq3(iter=2000,Z=Z,X=X,y=y,burnin=1000,tauu_0 = 2,a=a,g=g)
chain4 <- Gibbsq3(iter=2000,Z=Z,X=X,y=y,burnin=1000,tauu_0 = 5,a=a,g=g)

 

ml1<-as.mcmc.list(as.mcmc((chain1[1:500,])))
ml2<-as.mcmc.list(as.mcmc((chain2[1:500,])))
ml3<-as.mcmc.list(as.mcmc((chain3[1:500,])))
ml4<-as.mcmc.list(as.mcmc((chain4[1:500,])))
ml5<-as.mcmc.list(as.mcmc((chain1[500+1:500,])))
ml6<-as.mcmc.list(as.mcmc((chain2[500+1:500,])))
ml7<-as.mcmc.list(as.mcmc((chain4[500+1:500,])))
ml8<-as.mcmc.list(as.mcmc((chain4[500+1:500,])))
estml<-c(ml1,ml2,ml3,ml4,ml5,ml6,ml7,ml8)
```



```{r}
#Gelman-Rubin diagnostic.
gelman.diag(estml)[[1]]

#effective sample size.
effectiveSize(estml)
```

The Gelman-Rubin diagnostic showns most parameters have converged except sigma2_u as there Rhat value are sufficiently close to one.
```{r}
#Reporting posterior means and credible intervals.
#Means
colMeans(rbind(chain1,chain2,chain3,chain4)) 
#95 % central Credible interval
apply(rbind(chain1,chain2,chain3,chain4) ,2, FUN =function(x) quantile(x,c(0.025,0.975) ))
```






```{r}
# beta
for(i in 1:6){
  
  plot(chain1[,i],type='l',ylab=paste("beta",as.character(i)),col=1)
  lines(chain1[,i],type='l',col=1,ylab=expression(beta[i]))
  lines(chain2[,i],type='l',col=2,ylab=expression(beta[i]))
  lines(chain3[,i],type='l',col=3,ylab=expression(beta[i]))
  lines(chain4[,i],type='l',col=4,ylab=expression(beta[i]))
  legend('bottomright',legend=c('Chain 1','Chain 2','Chain 3','Chain 4'),col=1:4,lty=1,bty='n')}
```

```{r}
# u
for(i in 1:5){
  
  plot(chain1[,i+6],type='l',ylab=paste("u",as.character(i)),col=1)
  lines(chain1[,i+6],type='l',col=1)
  lines(chain2[,i+6],type='l',col=2)
  lines(chain3[,i+6],type='l',col=3)
  lines(chain4[,i+6],type='l',col=4)
  legend('topright',legend=c('Chain 1','Chain 2','Chain 3','Chain 4'),col=1:4,lty=1,bty='n')}
```

```{r}
# sigma2_u
plot(chain1[,12],type='l',ylab=paste("sigma2_u"),col=1)
lines(chain1[,12],type='l',col=1)
lines(chain2[,12],type='l',col=2)
lines(chain3[,12],type='l',col=3)
lines(chain4[,12],type='l',col=4)
legend('topright',legend=c('Chain 1','Chain 2','Chain 3','Chain 4'),col=1:4,lty=1,bty='n')
```


d) For the co-efficients $\bm \beta$, $\bf u$, calculate the mean of the ratio of the posterior means $\bm \beta_{i,\text{logit}}/\bm \beta_{i,\text{probit}}, {\bf u}_{i,\text{logit}}/{\bf u}_{i,\text{probit}}$ obtained when fitting the logistic mixed model and the probit mixed model. To do this, you will need to apply the \texttt{extract} function to the stan model object. Once calculated, multiply the iterations obtained assuming a probit link by this constant and compare to the iterations obtained assuming a logit link.  


Answer:
PART D 
```{r}
means_prob <- colMeans(rbind(chain1,chain2,chain3,chain4)) 
log_variables <- extract(logistic.mm)
means_log <- c(colMeans(log_variables$beta),colMeans(log_variables$u),mean(log_variables$sigma))
ratio <- means_log/means_prob
ratio
```
```{r}
# chains multiplied by the ratio

chain1s <- chain1*ratio
chain2s <- chain2*ratio
chain3s <- chain3*ratio
chain4s <- chain4*ratio


ml1s<-as.mcmc.list(as.mcmc((chain1[1:500,])))
ml2s<-as.mcmc.list(as.mcmc((chain2[1:500,])))
ml3s<-as.mcmc.list(as.mcmc((chain3[1:500,])))
ml4s<-as.mcmc.list(as.mcmc((chain4[1:500,])))
ml5s<-as.mcmc.list(as.mcmc((chain1[500+1:500,])))
ml6s<-as.mcmc.list(as.mcmc((chain2[500+1:500,])))
ml7s<-as.mcmc.list(as.mcmc((chain4[500+1:500,])))
ml8s<-as.mcmc.list(as.mcmc((chain4[500+1:500,])))
estmls<-c(ml1s,ml2s,ml3s,ml4s,ml5s,ml6s,ml7s,ml8s)
```

```{r}
gelman.diag(estml)[[1]]

#effective sample size.
effectiveSize(estmls)
```

```{r}
#Reporting posterior means and credible intervals.
#Means
colMeans(rbind(chain1s,chain2s,chain3s,chain4s)) 
#95 % central Credible interval
apply(rbind(chain1s,chain2s,chain3s,chain4s) ,2, FUN =function(x) quantile(x,c(0.025,0.975) ))
```

From the above report we can see that the posterior means of the probit link model multiplied by the ratio of posterior means are much closer to the log link model than the probit model alone. Also we see that all parameters converged except for sigma2_u.



e) The logistic link can be written in the same way as the probit link, but instead of $e_i\sim \mathcal{N}(0,1)$, the error term is $e_i \sim \text{Logistic}(0,1)$. By evaluating the standard normal and logistic inverse cdfs and superimposing the line $y = mx$ where $m$ is the posterior ratio, do you think the results in d) were surprising.

```{r}
p <- seq(from = 0,to = 1,length.out = 2000)
xn <- qnorm(p)
xl <- qlogis(p)

plot(xn,xl,col=1,xlab="normal quantiles",ylab="Logistic quantiles")
lines(xn,mean(ratio)*xn,type='l',col=2)
legend('topleft',legend=c('CDFs','y =mx'),col=1:2,lty=1,bty='n')
```

From the above q-q plot we see that the logistic link is the normal link scaled by the posterior ratio (however the logistic link has slightly heavier tails). Hence once the iterations were scaled it was not surprising to see the posterior means for both models get closer to each other.



