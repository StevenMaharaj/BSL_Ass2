---
title: 'Steven Maharaj 695281 Assignment 2, Question 3 MAST90125: Bayesian Statistical Learning'
header-includes:
   - \usepackage{bm}
   - \usepackage{amsmath}
   - \usepackage{amsfonts}
   - \usepackage{amssymb}
output: 
  pdf_document:
    number_sections: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Due: Friday 20 September 2019**  
\vspace{5 mm}

**There are places in this assignment where R code will be required. Therefore set the random seed so assignment is reproducible.**

```{r}
set.seed(695281)  #Please change random seed to your student id number.
library(dplyr)
library(ggplot2)
library(tidyr)
library(TruncatedNormal)
library(mvtnorm)
```


```{r}
rtn <- function(n,b,a,mu,Sigma){
  u <- runif(n)
  g <- pnorm((b-mu)/Sigma) - pnorm((a-mu)/Sigma)
  x <-qnorm((g) * u + pnorm((a-mu)/Sigma))*Sigma + mu 
}

```

___
PART C

We mplement a Gibbs sampler to fit the same mixed model, but now with a probit link.

Assumsing,

 - $p(\bm \beta) \propto 1$
 - $p(\bm u) = \mathcal{N}(\bm 0,\sigma^2_u \bm I)$
 - $p(\tau_u) = Ga(\alpha_u,\gamma_u)$
 
 It can be shown that we have the folling conditional posteriors
 
 $$p\left(\tau_{u} | \cdot\right)=\mathrm{Ga}\left(\alpha_{u}+q / 2, \gamma_{u}+\mathbf{u}^{\prime} \mathbf{u} / 2\right)$$

$$
\left.p\left(\left(\begin{array}{c}{\beta} \\ {u}\end{array}\right) | \cdot\right)=\mathcal{N}\left(\begin{array}{cc}{X^{\prime} X} & {X^{\prime} Z} \\ {Z^{\prime} X} & {Z^{\prime} Z+\tau_{u}  \bm{I}^{-1}}\end{array}\right)^{-1}\left(\begin{array}{c}{X^{\prime} z} \\ {Z^{\prime} z}\end{array}\right),\left(\begin{array}{cc}{X^{\prime} X} & {X'Z} \\ {Z^{\prime} X} & {Z'Z+\tau_{u}  \bm{I}^{-1}}\end{array}\right)^{-1}\right)
$$


We define our inputs for the Gibbs Sampler


```{r}
#Step one: Importing data,  constructing design matrices and calculating matrix dimensions.
dataX= read.csv("Contraceptionsubset.csv",header=TRUE)
n<-dim(dataX)[1]
Z    = table(1:n,dataX$district)     #incidence matrix for district
Q    = dim(Z)[2]
D1   = table(1:n,dataX$livch) #Dummy indicator for living children
D2   = table(1:n,dataX$urban) #Dummy indicator for urban status

#fixed effect design matrix 
X    = cbind(rep(1,n),dataX$age,D1[,-1],D2[,-1])
P    = dim(X)[2]
y    = rep(0,n)
y[dataX$use %in% 'Y'] = 1
a <- 0.001
g <- 0.001
```

Construct a Gibbs sampler
```{r}
Gibbsq3 <- function(iter,Z,X,y,burnin,tauu_0,a,g){
  q    = dim(Z)[2]
  p    = dim(X)[2]
  
  W<-cbind(X,Z)            #for the joint conditional posterior for b,u
  WTW <-crossprod(W)
  I0 <- diag(p+q)
  diag(I0)[1:p] <- 0
  
  #starting values.
  t_u <- tauu_0
  u   <-rnorm(q,0,sd=1/sqrt(t_u))
  
  uTu <- crossprod(u)
  
  #storing results.
  par <-matrix(0,iter,p+q+1)  
  
  for (i in 1:iter) {
    Prec <-WTW + t_u*I0
    
    t_u <- rgamma(1,a + q*0.5,g + uTu*0.5)
    z <- rtn(n=length(y),b=100,a=0,mu = 0.5,Sigma = 1)*(y==1) + rtn(n=length(y),b=0,a=-100,mu = 0.5,Sigma = 1)*(y==0) 
    P.mean <- solve(Prec)%*%crossprod(W,z)
    P.var  <-solve(Prec)
    res <- rmvnorm(1,P.mean,P.var)
    b <- res[1:p]
    u <- res[p+1:q]
    par[i,]<-c(b,u,1/t_u)
  }
par <-par[-c(1:burnin),] #removing initial iterations
colnames(par)<-c(paste('beta',1:p,sep=''),paste('u',1:q,sep=''),"sigma2_u")  
 return(par)
}
```

```{r}
res <- Gibbsq3(iter=2000,Z=Z,X=X,y=y,burnin=1000,tauu_0 = 1,a=a,g=g)
```

